// Este archivo manejar√° la l√≥gica para interactuar con la API de OpenAI, como la creaci√≥n de embeddings y generaci√≥n de respuestas.

import OpenAI from "openai";
import * as dotenv from "dotenv";

dotenv.config();

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY! });

// üéØ Prompt de comportamiento mejorado
const BEHAVIOR_PROMPT = `
Tu nombre es UTEChat y eres un asistente virtual de la Universidad Tecnol√≥gica UTEC. Eres especialista en la b√∫squeda de informaci√≥n dentro de la base de conocimiento de UTEC y en el almacenamiento de informaci√≥n relacionada con leads.

# Objetivo de UTEChat
Brindar atenci√≥n eficiente a estudiantes, docentes, analistas de carrera y colaboradores, generando respuestas basadas exclusivamente en la informaci√≥n proporcionada en la base de conocimiento. Aseg√∫rate de no inventar ni suponer informaci√≥n.

# Interacci√≥n Inicial
1. Si el usuario saluda (por ejemplo, "Hola", "Buen d√≠a", etc.), UTEChat debe iniciar la conversaci√≥n con este formato exacto:
   - "Hola, soy UTEChat. ¬øEn qu√© puedo ayudarte?"
2. Si el usuario realiza una consulta directamente, UTEChat debe responder siempre en la primera interacci√≥n con este formato exacto:
   - "Hola, soy UTEChat. Resolvamos tu consulta."
3. En todas las interacciones posteriores, UTEChat debe omitir el saludo inicial y utilizar √∫nicamente:
   - "Bien, resolvamos tu consulta."

# Estilo de Comunicaci√≥n
- Usa un tono profesional, amigable y cercano.
- Organiza la informaci√≥n con listas y t√≥picos.
- Usa emojis de forma moderada para mejorar la comprensi√≥n.
- Mant√©n las respuestas claras y concisas, con un m√°ximo de 250 palabras.

# Manejo de Consultas Complejas
Si la informaci√≥n recuperada no contiene una respuesta directa, indica de manera transparente que no tienes informaci√≥n suficiente y sugiere fuentes oficiales donde el usuario pueda obtener m√°s detalles.

# Normativas de Privacidad
UTEChat debe priorizar la seguridad y confidencialidad de la informaci√≥n de los usuarios en todo momento, cumpliendo con normativas como ISO 27001.
`;

// ‚úÖ Generar embeddings a partir de texto
export async function generateEmbeddings(text: string): Promise<number[]> {
  try {
    console.log("üìå Generando embeddings...");
    const response = await openai.embeddings.create({
      model: process.env.OPENAI_EMBEDDING_MODEL!,
      input: text,
    });

    return response.data[0].embedding;
  } catch (error) {
    console.error("‚ùå Error generando embeddings:", error);
    throw new Error("Error generando embeddings.");
  }
}

// ‚úÖ Generar respuesta con GPT optimizado
export async function generateResponse(userQuery: string, context: string = ""): Promise<string> {
  try {
    console.log("üìå Enviando consulta a OpenAI...");

    // üîç Filtrar y formatear el contexto
    const cleanContext = context.trim().length > 0
      ? `Aqu√≠ tienes la informaci√≥n disponible:\n\n${context}`
      : "No se encontraron datos relevantes para responder esta consulta.";

    // üî• Ajustamos din√°micamente el n√∫mero de tokens
    const tokenLimit = cleanContext.length > 1500 ? 700 : 500; // Balance entre precisi√≥n y costos

    const response = await openai.chat.completions.create({
      model: "gpt-4o",
      messages: [
        { role: "system", content: BEHAVIOR_PROMPT }, // üéØ Prompt base
        { role: "system", content: cleanContext }, // üîç Incluir contexto
        { role: "user", content: userQuery }, // ü§ñ Consulta del usuario
      ],
      max_tokens: tokenLimit, // üî• Mayor espacio para respuestas largas
      temperature: 0.4, // üìå Reducci√≥n de temperatura para mayor precisi√≥n
    });

    return response.choices[0]?.message?.content || "No tengo informaci√≥n suficiente.";
  } catch (error) {
    console.error("‚ùå Error generando respuesta con OpenAI:", error);
    return "Hubo un error generando la respuesta.";
  }
}
